---
title: "Practice: Simple Linear Regression"
author: 
  - "Selena Buttery"
  - "DS303, SP25"
  - "Prof. Amber Camp"
date: 1/20/25
format: html
editor: visual
theme: spacelab
---

## Introduction to Simple Linear Regression

This is an introduction to **simple linear regression**, a method used to model the relationship between two variables by fitting a straight line to the data. The goal is to see how one variable (the independent variable) affects another (the dependent variable).

For example, you might predict a student’s test score (dependent variable) based on study hours (independent variable). Simple linear regression helps find a trend or regression line that best fits the data, allowing you to make predictions for varying study hours.

Simple linear regression is useful for studying **cause-and-effect** or **making predictions**, especially when the relationship between the variables is linear. It works best with **continuous data**.

## *y* = *mx* + *b* ?

We talked a lot in class the other day about the basic regression equation. I presented it as:

*y* = *b~0~* + *b~1~x*

Many of you quickly noticed the similarity between *y* = *b~0~* + *b~1~x* and *y* = *mx* + *b.* And you're right–they are both basically the same formula for a straight line. Is there any actual difference at all?

Yes! Despite, again, basically being the same thing, the difference in notation depends on context (when we use the two and how we discuss them). See what you can find online about this, and feel free to discuss with those around you.

### Question 1

What is the difference between *y* = *b~0~* + *b~1~x* and *y* = *mx* + *b*, and when might we use one over the other? Please use your own words

#### Answer:

The main difference that i read about and researched was that basically in the equation *y* = *b~0~* + *b~1~x,* it is used more in context. So with statistics more than just basic algebra or geometry. The y=mx + b is mainly used when dealing with basic geometry problems that are linear.

### Question 2

Think back to our class discussion and your previous studies in math. Tell me below what each part of the equation *y* = *b~0~* + *b~1~x* means. Do this from memory if you can!

#### Answer:

*b*~0 = y intercept~

*b*~1 = slope~

x = independent variable

y = dependent variable

## Let's try it

Let's start by loading the `MASS` and `ISLR2` packages, which are very large collections of data sets and functions. You may need to install `ISLR2` and `lme4`.

```{r, echo = FALSE, message = FALSE}
# install.packages("ISLR2")
# install.packages("lme4")

library(MASS)
library(ISLR2)
library(lme4)
```

## Boston Housing Data

The `ISLR2` library contains the `Boston` data set, which records `medv` (median house value) for 506 census tracts in Boston. We will seek to predict `medv` using 12 predictors such as `rmvar` (average number of rooms per house), `age` (proportion of owner-occupied units built prior to 1940) and `lstat` (percent of households with low socioeconomic status).

### Question 3

You can just call upon the data (it's already in the package). I'll get you started, but show me below how you'd explore the data even further by adding code in the below code chunk.

```{r}
head(Boston)

summary(Boston)

hist(Boston$medv, breaks = 20, col = "blue", main = "Distribution of Median House Value", xlab = "medv")
boxplot(Boston$medv, main = "Boxplot of Median House Value", ylab = "medv")
```

We learned in class that we can apply a simple linear regression using `lm`. Here is the basic format:

```{r}
model <- lm(y ~ x, data=df)
```

### Question 4

Use the above basic format to create a linear regression model to predict the **median home value** (medv) based on the **percentage of lower status population** (lstat), using the data from the 'Boston' dataset. Assign it to the variable `lm.model`.

```{r}
lm.model <- lm(medv ~ lstat, data = Boston)
```

If you set it up right, you should be able to run your model name in the below code chunk and view the basic model output. Give it a try:

```{r}
lm.model
```

### Question 5

What is your model output telling you?

#### Answer

The output is telling me that the y-intercept is 34.55 which means that this is the predicted value of medv when lstat is 0. In this example it would mean that if there was a 0% lower-status population then the median value of a home would be \$34,550. Since the slope is -.95, this tells us that for each 0.01 increase in lower-status population, the median home value would decrease by \$950.

You can also try `summary(lm.model)`.

```{r}
summary(lm.model)
```

### Question 6

What additional information do you get from this summary of the model output?

#### Answer

We get the five number summary of the residuals which are the min, first quartile, median, third quartile, and max. We can also see the residual standard error (RSE) which is explained as the average difference of the observed value from the regression line. We can also see that for every increase in lstat, medv goes down by 0.95 as shown under the estimate for lstat. We can also see R\^2 which means approximately 54.41% of the variability in medv is explained by lstat. And lastly there is the F statistic which tests whether the model is statistically significant overall. We also can see the p-value which is very low according to the summary results which means that the probability of the observed data is statistically significant. Overall this output shows us that this is a good model and lstat predicts medv.

## confint() and predict()

In order to obtain a confidence interval for the coefficient estimates, we can use the `confint()` command. The `predict()` function can be used to produce confidence intervals and prediction intervals for the prediction of `medv` for a given value of `lstat`. Run these and see if you can figure out what it is telling you.

```{r}
confint(lm.model)

predict(lm.model, data.frame(lstat = (c(5, 10, 15))), interval = "confidence")

predict(lm.model, data.frame(lstat = (c(5, 10, 15))), interval = "prediction")
```

### Question 7

What do you think the above `confint()` and `predict()` information means? It's okay to guess.

#### Answer

I think that for the intercept (33.44 - 35.65) it is telling us that when lstat = 0 then the expected medv value is approx between 33.44 and 35.65. I believe that for the Confint() information it is focused on the prediction based on the 95% confidence and it shows the predicted medv with the lower and upper bounds. And then for the prediction it is showing the variability in the individual outcomes that is why we see a more broad range I believe.

## Visualizing Question 8

Can you convert the above code chunk to `ggplot`? Try below. Have fun with editing the appearance of your plot if you'd like :)

```{r}
library(ggplot2)

confidence_preds <- predict(lm.model, data.frame(lstat = c(5, 10, 15)), interval = "confidence")
prediction_preds <- predict(lm.model, data.frame(lstat = c(5, 10, 15)), interval = "prediction")


lstat_values <- c(5, 10, 15)
data <- data.frame(
  lstat = rep(lstat_values, 2),
  fit = c(confidence_preds[, "fit"], prediction_preds[, "fit"]),
  lwr = c(confidence_preds[, "lwr"], prediction_preds[, "lwr"]),
  upr = c(confidence_preds[, "upr"], prediction_preds[, "upr"]),
  type = rep(c("Confidence", "Prediction"), each = length(lstat_values))
)

library(ggplot2)
ggplot(data, aes(x = factor(lstat), y = fit, fill = type)) +
  geom_col(position = "dodge", alpha = 0.7) + # Bars for predictions
  labs(
    title = "Confidence and Prediction Intervals",
    x = "LSTAT",
    y = "Predicted MEDV",
    fill = "Interval Type"
  ) +
  theme_minimal()
```

```{r}

```

```         
```

In a future class, we'll explore some diagnostic plots and what that means for evaluating models. For now, just run the below and have a look:

```{r}
par(mfrow = c(2, 2))
plot(lm.model)
```

## Run another model

Now it's your turn to apply a linear regression to other variables from the Boston dataset.

First, view the dataset. Where can you find information about these variables?

```{r}
View(Boston)
```

### Question 9

What variables are you interested in exploring using a linear regression? Just pick and `x` and a `y` and describe your research question below in plain English:

#### Answer

X = Age (proportion of owner-occupied units built before 1940s

Y = Lstat (percentage of households with low socioeconomic status)

Question: Does the proportion of older homes in a neighborhood correlate with the percentage of households with low socioeconomic status?

### Question 10

#### Part 1

Build and run your model, examine the model output:

```{r}
model_practice <- lm(lstat ~ age, data = Boston)
```

```{r}
model_practice

summary(model_practice)
```

```{r}
confint(model_practice)
predict(model_practice, data.frame(age = c(20, 50, 80)), interval = "confidence")
predict(model_practice, data.frame(age = c(20, 50, 80)), interval = "prediction")
```

#### Part 2

Explain what you found in plain English. Do your best.

#### Answer

What I have found is that typically older neighborhoods tend to have a higher percentage of lower socioeconomic households, but there is significant variation among individual households. I found out the first part by taking the information and seeing that for every 1% increase in older homes, lstat increases by 0.14 to 0.17 on average.

## The end!

That's it for now. Please feel free to raise questions in class or via email!

### Feb 3 Classwork

Create a multiple regression model that includes an interaction term and explain the model output in plain language below. Save, commit, and push to GitHub when done.

```{r}
lm.class <- lm(medv ~ age * crim, data = Boston)
summary(lm.class)
```

This is telling us that when age = 0, then medv is decreasing by 0.09 and when crim = 0, medv is decreasing by 1.16. And then when both age and crim =0 then medv increases by .009. This model is statistically significant according to our p values as well.
