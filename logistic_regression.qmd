---
title: "Logistic Regression"
author: 
  - "Selena Buttery"
  - "DS303, SP25"
  - "Prof. Amber Camp"
date: 2/24/25
format: html
editor: visual
theme: spacelab
---

## Logistic Regression

You know all about linear regression now! Let's talk about logistic regression. Logistic regression is a type of regression analysis used when the dependent variable is binary (e.g., success/failure, yes/no, high/low, green/blue). It models the **probability** that a given input point belongs to a particular category.

## Log Odds

Logistic regression relies on log-odds to model the relationship between predictor variables and a binary outcome

-   **Odds** are a way to compare the likelihood of an event occurring to the likelihood of it not occurring.

-   **Log-odds** transforms the odds into a logarithmic scale.

Log-odds convert probabilities into a continuous scale, enabling a linear relationship between predictors and the likelihood of an event. This transformation is key for understanding and interpreting logistic regression models.

## Why not just use linear regression?

**Linear regression** is not suitable for binary outcomes because:

-   It can predict values outside \[0, 1\]

-   It assumes a linear relationship, while binary outcomes follow an S-shaped curve

The logistic function, or sigmoid function, solves this by mapping any real number to (0, 1).

## Today: Practical stuff first

Focus will be on the practical application. We may cover a bit more in-depth theory on Wednesday.

By the end of today's lesson, students will be able to:

-   Understand the concept of logistic regression and its use cases

-   Interpret logistic regression coefficients

-   Evaluate model fit and performance

-   Implement logistic regression in R

## Load Packages

Standard suspects: `lme4` and `tidyverse`. For this exercise, we'll use the `Default` data from the `ISLR2` package.

```{r, echo = FALSE, message = FALSE}
# install.packages("ISLR2")
# install.packages("lme4")

library(lme4)
library(tidyverse)
library(ISLR2)
```

## Load & Explore Data

```{r}
default <- Default
skim_data <- skimr::skim(default)
View(skim_data)

?Default
```

What is this data showing? Which data is binary here?

Answer: Credit card default data, binary variables include default and student.

## Basic logistic regression syntax

Here is the basic model.

```{r}
# logistic regression
model <- glm(y ~ x, data = df, family = binomial)

# make sure you are using glm(), not lm()
# make sure you include family = binomial to constrain probabilities between 0 and 1 and transform the probabilities to the log-odds scale.
```

## Visualize first

Explore the data visually.

```{r}
ggplot(default, aes(x = student, y = income)) +
  geom_point(color = "blue") + 
  labs(title = "Scatter Plot", x = "Student", y = "Income") +
  theme_minimal()

```

## Apply the logistic regression

Edit the below to predict `student` status based on `income`.

```{r}
student <- glm(student ~ income, data = default, family = binomial)
summary(student)
```

Interpretation of Estimate coefficients:

-   **(Intercept)**: 9.436

    -   This is the log-odds of being a student when income is zero.

-   **income**: -3.945e-04 (or -0.0003945)

    -   This coefficient represents the change in the log-odds of being a student for a one-unit increase in income. Since the coefficient is negative, it suggests that **as income increases, the log-odds of being a student decreases**.

### Include `default`

```{r}
student2 <- glm(student ~ income + default, data = default, family = binomial)
summary(student2)
```

Interpretation of Estimate coefficients:

-   **defaultYes**: 3.828e-01 (or 0.3828)

    -   Indicates that being in the "Yes" category for `default` (i.e., having defaulted) is associated with an increase of approximately 0.3828 in the log-odds of being a student, when compared to `defaultNo` (those who have not defaulted). This effect is not significant, however.

### Try another

Edit the below to predict `default` status based on `income`.

```{r}
model <- glm(default ~ income, data = default, family = binomial)
summary (model)
```

Interpret the results below:

-   **(Intercept)**: -3.094e+00

    -   This is the log odds of default when income is 0.

-   Income: -8.353e-06

    -   This coefficient represents the change in the log-odds of defaulting for a one-unit increase in income. Since the coefficient is negative, it suggests that **as income increases, the log-odds of being defaulting decreases**.

## Model comparison

Choose either `student` or `default` and try to build the optimal model that predicts that status. Start by including all other terms, and then perform model comparison to find the best model to predict the status you chose.

```{r}
best_model <- glm(default ~ income + balance + student, data = default, family = binomial)

summary(best_model)
```

```{r}
best_model2 <- glm(student ~ income + balance + default, data = default, family = binomial)

summary(best_model2)
```

Interpret your findings here:

So I think that the bestmodel2 is the better model to use due to all of the variables becoming statistically significant when comparing them with the student variable. When I tried to compare them with default group, income was not statistically significant which makes that model not as good as this second one I built.

I found that the intercept which is 8.526 is the log odds of student when income, balance, and default all equal zero.

**Income: -3.933e-04**

As income increases, the log odds of being a student decreases.

**Balance: 1.024e-03**

As balance increases, the log odds of being a student increases.

**DefaultYes: -5.699e-01**

If the person has defaulted, the log odds of them being a student decreases.

## Was that too easy?

Load `mtcars` and build a model to predict whether transmission is automatic or manual. You will have to perform a transformation on the transmission data first.

```{r}
mtcars <- mtcars

?mtcars # use this to remind yourself of which variables are present

# transform transmission data to correct type


# build model
```
